{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "088ec08c-d677-426b-aa50-e1b2007fb393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import random\n",
    "from random import sample\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61260330-bf50-45a6-ac05-0e50d304ad4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: runs/tcga*\n"
     ]
    }
   ],
   "source": [
    "!rm -rf runs/tcga*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fb5d5e-8533-4cd3-87c4-7cce50c13d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tcga_tybalt_file_location = 'data/pancan_scaled_zeroone_rnaseq.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee6a57f-c5ed-4524-ac85-e7b3bbd06f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10459, 5000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.678296</td>\n",
       "      <td>0.289910</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.746797</td>\n",
       "      <td>0.687833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440610</td>\n",
       "      <td>0.428782</td>\n",
       "      <td>0.732819</td>\n",
       "      <td>0.634340</td>\n",
       "      <td>0.580662</td>\n",
       "      <td>0.294313</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>0.478219</td>\n",
       "      <td>0.168263</td>\n",
       "      <td>0.638497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200633</td>\n",
       "      <td>0.654917</td>\n",
       "      <td>0.181993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100606</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.092586</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.140642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620658</td>\n",
       "      <td>0.363207</td>\n",
       "      <td>0.592269</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.610192</td>\n",
       "      <td>0.374569</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.271356</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.602560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RPS4Y1      XIST      KRT5  AGR2  CEACAM5     KRT6A     KRT14   CEACAM6  \\\n",
       "0  0.678296  0.289910  0.034230   0.0      0.0  0.084731  0.031863  0.037709   \n",
       "1  0.200633  0.654917  0.181993   0.0      0.0  0.100606  0.050011  0.092586   \n",
       "\n",
       "      DDX3Y     KDM5D  ...   FAM129A   C8orf48    CDK5R1    FAM81A  C13orf18  \\\n",
       "0  0.746797  0.687833  ...  0.440610  0.428782  0.732819  0.634340  0.580662   \n",
       "1  0.103725  0.140642  ...  0.620658  0.363207  0.592269  0.602755  0.610192   \n",
       "\n",
       "      GDPD3     SMAGP   C2orf85   POU5F1B     CHST2  \n",
       "0  0.294313  0.458134  0.478219  0.168263  0.638497  \n",
       "1  0.374569  0.722420  0.271356  0.160465  0.602560  \n",
       "\n",
       "[2 rows x 5000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnaseq_df = pd.read_table(tcga_tybalt_file_location)\n",
    "rnaseq_df.drop(columns=rnaseq_df.columns[0], axis=1,  inplace=True)\n",
    "rnaseq_df = rnaseq_df.dropna()\n",
    "print(rnaseq_df.shape)\n",
    "rnaseq_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7afa172e-c31d-4de1-96e2-7afe6a96a8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set_percent = 0.2\n",
    "rnaseq_df_test = rnaseq_df.sample(frac=test_set_percent)\n",
    "rnaseq_df_train = rnaseq_df.drop(rnaseq_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c7553cd-bc1e-4ea1-a026-8ffceca9d9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx].values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b88d241-c51d-447c-ab2e-35c06a1a9e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(rnaseq_df_train)\n",
    "test_dataset = CustomDataset(rnaseq_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65571fe1-7693-4f1b-a809-8e3df56e0d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=50, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99018251-fbe3-41df-8d9d-6f28e96c05f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: list, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dim[0])])\n",
    "        self.decoder_layers = nn.ModuleList([nn.Linear(hidden_dim[0], input_dim)])\n",
    "                \n",
    "        if len(hidden_dim)>1:\n",
    "            for i in range(len(hidden_dim)-1):\n",
    "                self.encoder_layers.append(nn.Linear(hidden_dim[i], hidden_dim[i+1]))\n",
    "                self.decoder_layers.insert(0, nn.Linear(hidden_dim[i+1], hidden_dim[i]))\n",
    "                \n",
    "        self.encoder_layers.append(nn.Linear(hidden_dim[-1], 2 * z_dim))\n",
    "        self.batchnorm = nn.BatchNorm1d(z_dim)\n",
    "        self.decoder_layers.insert(0, nn.Linear(z_dim, hidden_dim[-1]))\n",
    "\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        for idx, layer in enumerate(self.encoder_layers):\n",
    "            x = layer(x)\n",
    "            if idx < len(self.encoder_layers) - 1:\n",
    "                # x = F.dropout(x, 0.01)\n",
    "                x = F.relu(x)\n",
    "                #x = nn.BatchNorm1d(x)\n",
    "        return x[...,:self.z_dim], x[...,self.z_dim:] # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        # std = torch.abs(log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        for idx, layer in enumerate(self.decoder_layers):\n",
    "            z = layer(z)\n",
    "            if idx < len(self.decoder_layers) - 1:\n",
    "                # x = F.dropout(x, 0.01)\n",
    "                z = F.relu(z)\n",
    "        return torch.sigmoid(z) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, input_dim))\n",
    "        mu = self.batchnorm(mu)\n",
    "        log_var = self.batchnorm(log_var)\n",
    "    #    z = self.sampling(mu, log_var)\n",
    "        latent = MultivariateNormal(loc = mu, \n",
    "                                    scale_tril=torch.diag_embed(torch.exp(0.5*log_var)))\n",
    "        z = latent.rsample()\n",
    "           \n",
    "    #    return self.decoder(z), mu, log_var\n",
    "        return self.decoder(z), latent\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(recon_x, x, mu, log_var):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_function_dist(recon_x, x, latent, input_dim):\n",
    "        prior = MultivariateNormal(loc = torch.zeros(latent.mean.shape[1]),\n",
    "                                   scale_tril=torch.eye(latent.mean.shape[1]))\n",
    "        \n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = torch.sum(kl_divergence(latent, prior))\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888f9cb-5a1a-487a-a341-d603d6a8bef3",
   "metadata": {},
   "source": [
    "VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c02e248-d6bb-4137-9fe7-72bb541f8cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        # Every data instance \n",
    "        data = data.to(DEVICE)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        recon_batch, latent = vae(data)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = VAE.loss_function_dist(recon_batch, data, latent, input_dim)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100.0 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c7fc8c8-8d2f-46e8-9a56-044ae0e3dc8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 159190.97359375\n",
      "LOSS train 159190.97359375 valid 186473.984375\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 144129.82484375\n",
      "LOSS train 144129.82484375 valid 158527.640625\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 141895.46359375\n",
      "LOSS train 141895.46359375 valid 151911.84375\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 140772.69609375\n",
      "LOSS train 140772.69609375 valid 151951.03125\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 140114.9878125\n",
      "LOSS train 140114.9878125 valid 148374.75\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 139427.70390625\n",
      "LOSS train 139427.70390625 valid 148817.921875\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 139188.25890625\n",
      "LOSS train 139188.25890625 valid 147931.75\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 138941.07171875\n",
      "LOSS train 138941.07171875 valid 147887.828125\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 138843.95921875\n",
      "LOSS train 138843.95921875 valid 146943.671875\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 138710.41921875\n",
      "LOSS train 138710.41921875 valid 147469.421875\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 138632.77\n",
      "LOSS train 138632.77 valid 148356.59375\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 138530.9675\n",
      "LOSS train 138530.9675 valid 148051.40625\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 138316.045\n",
      "LOSS train 138316.045 valid 148100.203125\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 138274.03609375\n",
      "LOSS train 138274.03609375 valid 147610.765625\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 138280.36171875\n",
      "LOSS train 138280.36171875 valid 148554.640625\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 138184.165\n",
      "LOSS train 138184.165 valid 147937.8125\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 138072.06421875\n",
      "LOSS train 138072.06421875 valid 148088.8125\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 137996.35328125\n",
      "LOSS train 137996.35328125 valid 148150.453125\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 138103.0559375\n",
      "LOSS train 138103.0559375 valid 148032.296875\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 138035.47125\n",
      "LOSS train 138035.47125 valid 148674.328125\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "input_dim=rnaseq_df.shape[1]\n",
    "#vae = VAE(input_dim=input_dim, hidden_dim=[100,100], z_dim=100)\n",
    "vae = VAE(input_dim=input_dim, hidden_dim=[100], z_dim=100)\n",
    "# if torch.backends.mps.is_available():\n",
    "#     DEVICE = 'mps'\n",
    "# else:\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(torch.randn(30, 5000)), batch_size=100, shuffle=True)\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "    \n",
    "vae.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/tcga_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    vae.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    vae.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs = vdata\n",
    "        voutputs, latent = vae(vinputs)\n",
    "        \n",
    "        vloss = VAE.loss_function_dist(voutputs, vinputs, latent, input_dim)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        #torch.save(vae.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8752fd-73ca-4920-84e7-7279a81909f5",
   "metadata": {},
   "source": [
    "Shuffle the colums of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e02ee5c2-0a73-4272-b686-ba5cffc6f127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe0775-60c6-404b-b898-5314d4181cbb",
   "metadata": {},
   "source": [
    "BioBomb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f473677-ec3b-43d5-8432-99fcb6ddd5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnaseq_df_test.to_csv('data/rnaseq_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27c6d509-1e68-405b-812f-63eeb5840061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.711395</td>\n",
       "      <td>0.079027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>0.121969</td>\n",
       "      <td>0.062543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572671</td>\n",
       "      <td>0.512329</td>\n",
       "      <td>0.461674</td>\n",
       "      <td>0.491376</td>\n",
       "      <td>0.501528</td>\n",
       "      <td>0.495456</td>\n",
       "      <td>0.644358</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.429427</td>\n",
       "      <td>0.582995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10150</th>\n",
       "      <td>0.781910</td>\n",
       "      <td>0.031101</td>\n",
       "      <td>0.684937</td>\n",
       "      <td>0.700038</td>\n",
       "      <td>0.230084</td>\n",
       "      <td>0.254548</td>\n",
       "      <td>0.416508</td>\n",
       "      <td>0.361120</td>\n",
       "      <td>0.781960</td>\n",
       "      <td>0.786848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659345</td>\n",
       "      <td>0.208477</td>\n",
       "      <td>0.458558</td>\n",
       "      <td>0.505048</td>\n",
       "      <td>0.454214</td>\n",
       "      <td>0.501694</td>\n",
       "      <td>0.586376</td>\n",
       "      <td>0.263308</td>\n",
       "      <td>0.332541</td>\n",
       "      <td>0.388225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7324</th>\n",
       "      <td>0.788988</td>\n",
       "      <td>0.079403</td>\n",
       "      <td>0.457493</td>\n",
       "      <td>0.680465</td>\n",
       "      <td>0.366980</td>\n",
       "      <td>0.285862</td>\n",
       "      <td>0.310623</td>\n",
       "      <td>0.352631</td>\n",
       "      <td>0.769542</td>\n",
       "      <td>0.737494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293865</td>\n",
       "      <td>0.247636</td>\n",
       "      <td>0.407056</td>\n",
       "      <td>0.375633</td>\n",
       "      <td>0.160404</td>\n",
       "      <td>0.778409</td>\n",
       "      <td>0.753785</td>\n",
       "      <td>0.043088</td>\n",
       "      <td>0.535991</td>\n",
       "      <td>0.555555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>0.747995</td>\n",
       "      <td>0.046092</td>\n",
       "      <td>0.130644</td>\n",
       "      <td>0.177930</td>\n",
       "      <td>0.129670</td>\n",
       "      <td>0.115420</td>\n",
       "      <td>0.039233</td>\n",
       "      <td>0.157857</td>\n",
       "      <td>0.729496</td>\n",
       "      <td>0.689956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479202</td>\n",
       "      <td>0.082720</td>\n",
       "      <td>0.480683</td>\n",
       "      <td>0.638266</td>\n",
       "      <td>0.464715</td>\n",
       "      <td>0.302999</td>\n",
       "      <td>0.599158</td>\n",
       "      <td>0.169778</td>\n",
       "      <td>0.289458</td>\n",
       "      <td>0.360866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3477</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798650</td>\n",
       "      <td>0.712625</td>\n",
       "      <td>0.560407</td>\n",
       "      <td>0.061522</td>\n",
       "      <td>0.166504</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.331431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799483</td>\n",
       "      <td>0.487555</td>\n",
       "      <td>0.380651</td>\n",
       "      <td>0.409978</td>\n",
       "      <td>0.531780</td>\n",
       "      <td>0.534171</td>\n",
       "      <td>0.609271</td>\n",
       "      <td>0.146475</td>\n",
       "      <td>0.193681</td>\n",
       "      <td>0.412169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7975</th>\n",
       "      <td>0.736170</td>\n",
       "      <td>0.152171</td>\n",
       "      <td>0.476311</td>\n",
       "      <td>0.702972</td>\n",
       "      <td>0.768507</td>\n",
       "      <td>0.343426</td>\n",
       "      <td>0.281088</td>\n",
       "      <td>0.758171</td>\n",
       "      <td>0.760010</td>\n",
       "      <td>0.740522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728246</td>\n",
       "      <td>0.450068</td>\n",
       "      <td>0.435667</td>\n",
       "      <td>0.488870</td>\n",
       "      <td>0.465533</td>\n",
       "      <td>0.509826</td>\n",
       "      <td>0.706824</td>\n",
       "      <td>0.165557</td>\n",
       "      <td>0.337093</td>\n",
       "      <td>0.427780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8056</th>\n",
       "      <td>0.038897</td>\n",
       "      <td>0.759848</td>\n",
       "      <td>0.311547</td>\n",
       "      <td>0.646421</td>\n",
       "      <td>0.384474</td>\n",
       "      <td>0.253147</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>0.431012</td>\n",
       "      <td>0.077446</td>\n",
       "      <td>0.045757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654467</td>\n",
       "      <td>0.365170</td>\n",
       "      <td>0.405802</td>\n",
       "      <td>0.517247</td>\n",
       "      <td>0.563040</td>\n",
       "      <td>0.391213</td>\n",
       "      <td>0.665276</td>\n",
       "      <td>0.427613</td>\n",
       "      <td>0.196777</td>\n",
       "      <td>0.596736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8791</th>\n",
       "      <td>0.530575</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>0.332931</td>\n",
       "      <td>0.694337</td>\n",
       "      <td>0.931342</td>\n",
       "      <td>0.262074</td>\n",
       "      <td>0.335299</td>\n",
       "      <td>0.671591</td>\n",
       "      <td>0.751476</td>\n",
       "      <td>0.763546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367071</td>\n",
       "      <td>0.262613</td>\n",
       "      <td>0.361214</td>\n",
       "      <td>0.661306</td>\n",
       "      <td>0.897802</td>\n",
       "      <td>0.482602</td>\n",
       "      <td>0.739770</td>\n",
       "      <td>0.044569</td>\n",
       "      <td>0.468041</td>\n",
       "      <td>0.341742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>0.764453</td>\n",
       "      <td>0.174147</td>\n",
       "      <td>0.824906</td>\n",
       "      <td>0.453406</td>\n",
       "      <td>0.608007</td>\n",
       "      <td>0.842768</td>\n",
       "      <td>0.765355</td>\n",
       "      <td>0.648007</td>\n",
       "      <td>0.779927</td>\n",
       "      <td>0.729218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478843</td>\n",
       "      <td>0.438191</td>\n",
       "      <td>0.567193</td>\n",
       "      <td>0.274560</td>\n",
       "      <td>0.363217</td>\n",
       "      <td>0.512191</td>\n",
       "      <td>0.693135</td>\n",
       "      <td>0.110666</td>\n",
       "      <td>0.242758</td>\n",
       "      <td>0.675748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.742199</td>\n",
       "      <td>0.256187</td>\n",
       "      <td>0.709104</td>\n",
       "      <td>0.702332</td>\n",
       "      <td>0.213636</td>\n",
       "      <td>0.072746</td>\n",
       "      <td>0.801657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654467</td>\n",
       "      <td>0.434575</td>\n",
       "      <td>0.495473</td>\n",
       "      <td>0.558814</td>\n",
       "      <td>0.534143</td>\n",
       "      <td>0.368263</td>\n",
       "      <td>0.601515</td>\n",
       "      <td>0.239620</td>\n",
       "      <td>0.312022</td>\n",
       "      <td>0.468389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2092 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         RPS4Y1      XIST      KRT5      AGR2   CEACAM5     KRT6A     KRT14  \\\n",
       "2018   0.000000  0.711395  0.079027  0.000000  0.000000  0.023197  0.121969   \n",
       "10150  0.781910  0.031101  0.684937  0.700038  0.230084  0.254548  0.416508   \n",
       "7324   0.788988  0.079403  0.457493  0.680465  0.366980  0.285862  0.310623   \n",
       "2055   0.747995  0.046092  0.130644  0.177930  0.129670  0.115420  0.039233   \n",
       "3477   0.000000  0.798650  0.712625  0.560407  0.061522  0.166504  0.664300   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "7975   0.736170  0.152171  0.476311  0.702972  0.768507  0.343426  0.281088   \n",
       "8056   0.038897  0.759848  0.311547  0.646421  0.384474  0.253147  0.215385   \n",
       "8791   0.530575  0.151500  0.332931  0.694337  0.931342  0.262074  0.335299   \n",
       "1265   0.764453  0.174147  0.824906  0.453406  0.608007  0.842768  0.765355   \n",
       "1792   0.000000  0.742199  0.256187  0.709104  0.702332  0.213636  0.072746   \n",
       "\n",
       "        CEACAM6     DDX3Y     KDM5D  ...   FAM129A   C8orf48    CDK5R1  \\\n",
       "2018   0.062543  0.000000  0.000000  ...  0.572671  0.512329  0.461674   \n",
       "10150  0.361120  0.781960  0.786848  ...  0.659345  0.208477  0.458558   \n",
       "7324   0.352631  0.769542  0.737494  ...  0.293865  0.247636  0.407056   \n",
       "2055   0.157857  0.729496  0.689956  ...  0.479202  0.082720  0.480683   \n",
       "3477   0.331431  0.000000  0.000000  ...  0.799483  0.487555  0.380651   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "7975   0.758171  0.760010  0.740522  ...  0.728246  0.450068  0.435667   \n",
       "8056   0.431012  0.077446  0.045757  ...  0.654467  0.365170  0.405802   \n",
       "8791   0.671591  0.751476  0.763546  ...  0.367071  0.262613  0.361214   \n",
       "1265   0.648007  0.779927  0.729218  ...  0.478843  0.438191  0.567193   \n",
       "1792   0.801657  0.000000  0.088777  ...  0.654467  0.434575  0.495473   \n",
       "\n",
       "         FAM81A  C13orf18     GDPD3     SMAGP   C2orf85   POU5F1B     CHST2  \n",
       "2018   0.491376  0.501528  0.495456  0.644358  0.113569  0.429427  0.582995  \n",
       "10150  0.505048  0.454214  0.501694  0.586376  0.263308  0.332541  0.388225  \n",
       "7324   0.375633  0.160404  0.778409  0.753785  0.043088  0.535991  0.555555  \n",
       "2055   0.638266  0.464715  0.302999  0.599158  0.169778  0.289458  0.360866  \n",
       "3477   0.409978  0.531780  0.534171  0.609271  0.146475  0.193681  0.412169  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "7975   0.488870  0.465533  0.509826  0.706824  0.165557  0.337093  0.427780  \n",
       "8056   0.517247  0.563040  0.391213  0.665276  0.427613  0.196777  0.596736  \n",
       "8791   0.661306  0.897802  0.482602  0.739770  0.044569  0.468041  0.341742  \n",
       "1265   0.274560  0.363217  0.512191  0.693135  0.110666  0.242758  0.675748  \n",
       "1792   0.558814  0.534143  0.368263  0.601515  0.239620  0.312022  0.468389  \n",
       "\n",
       "[2092 rows x 5000 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnaseq_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d52df-ed47-4777-91e8-f4514fcaa242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
