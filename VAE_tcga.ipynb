{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088ec08c-d677-426b-aa50-e1b2007fb393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import random\n",
    "from random import sample\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fb5d5e-8533-4cd3-87c4-7cce50c13d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tcga_tybalt_file_location = 'data/pancan_scaled_zeroone_rnaseq.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee6a57f-c5ed-4524-ac85-e7b3bbd06f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10459, 5000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.678296</td>\n",
       "      <td>0.289910</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.746797</td>\n",
       "      <td>0.687833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440610</td>\n",
       "      <td>0.428782</td>\n",
       "      <td>0.732819</td>\n",
       "      <td>0.634340</td>\n",
       "      <td>0.580662</td>\n",
       "      <td>0.294313</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>0.478219</td>\n",
       "      <td>0.168263</td>\n",
       "      <td>0.638497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200633</td>\n",
       "      <td>0.654917</td>\n",
       "      <td>0.181993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100606</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.092586</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.140642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620658</td>\n",
       "      <td>0.363207</td>\n",
       "      <td>0.592269</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.610192</td>\n",
       "      <td>0.374569</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.271356</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.602560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RPS4Y1      XIST      KRT5  AGR2  CEACAM5     KRT6A     KRT14   CEACAM6  \\\n",
       "0  0.678296  0.289910  0.034230   0.0      0.0  0.084731  0.031863  0.037709   \n",
       "1  0.200633  0.654917  0.181993   0.0      0.0  0.100606  0.050011  0.092586   \n",
       "\n",
       "      DDX3Y     KDM5D  ...   FAM129A   C8orf48    CDK5R1    FAM81A  C13orf18  \\\n",
       "0  0.746797  0.687833  ...  0.440610  0.428782  0.732819  0.634340  0.580662   \n",
       "1  0.103725  0.140642  ...  0.620658  0.363207  0.592269  0.602755  0.610192   \n",
       "\n",
       "      GDPD3     SMAGP   C2orf85   POU5F1B     CHST2  \n",
       "0  0.294313  0.458134  0.478219  0.168263  0.638497  \n",
       "1  0.374569  0.722420  0.271356  0.160465  0.602560  \n",
       "\n",
       "[2 rows x 5000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnaseq_df = pd.read_table(tcga_tybalt_file_location)\n",
    "rnaseq_df.drop(columns=rnaseq_df.columns[0], axis=1,  inplace=True)\n",
    "rnaseq_df = rnaseq_df.dropna()\n",
    "print(rnaseq_df.shape)\n",
    "rnaseq_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afa172e-c31d-4de1-96e2-7afe6a96a8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set_percent = 0.1\n",
    "rnaseq_df_test = rnaseq_df.sample(frac=test_set_percent)\n",
    "rnaseq_df_train = rnaseq_df.drop(rnaseq_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c7553cd-bc1e-4ea1-a026-8ffceca9d9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx].values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b88d241-c51d-447c-ab2e-35c06a1a9e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(rnaseq_df_train)\n",
    "test_dataset = CustomDataset(rnaseq_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65571fe1-7693-4f1b-a809-8e3df56e0d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99018251-fbe3-41df-8d9d-6f28e96c05f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: list, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dim[0])])\n",
    "        self.decoder_layers = nn.ModuleList([nn.Linear(hidden_dim[0], input_dim)])\n",
    "                \n",
    "        if len(hidden_dim)>1:\n",
    "            for i in range(len(hidden_dim)-1):\n",
    "                self.encoder_layers.append(nn.Linear(hidden_dim[i], hidden_dim[i+1]))\n",
    "                self.decoder_layers.insert(0, nn.Linear(hidden_dim[i+1], hidden_dim[i]))\n",
    "                \n",
    "        self.encoder_layers.append(nn.Linear(hidden_dim[-1], 2 * z_dim))\n",
    "        self.batchnorm = nn.BatchNorm1d(z_dim)\n",
    "        self.decoder_layers.insert(0, nn.Linear(z_dim, hidden_dim[-1]))\n",
    "\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        for idx, layer in enumerate(self.encoder_layers):\n",
    "            x = layer(x)\n",
    "            if idx < len(self.encoder_layers) - 1:\n",
    "                # x = F.dropout(x, 0.01)\n",
    "                x = F.relu(x)\n",
    "                #x = nn.BatchNorm1d(x)\n",
    "        return x[...,:self.z_dim], x[...,self.z_dim:] # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        # std = torch.abs(log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        for idx, layer in enumerate(self.decoder_layers):\n",
    "            z = layer(z)\n",
    "            if idx < len(self.decoder_layers) - 1:\n",
    "                # x = F.dropout(x, 0.01)\n",
    "                z = F.relu(z)\n",
    "        return torch.sigmoid(z) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, input_dim))\n",
    "        mu = self.batchnorm(mu)\n",
    "        log_var = self.batchnorm(log_var)\n",
    "    #    z = self.sampling(mu, log_var)\n",
    "        latent = MultivariateNormal(loc = mu, \n",
    "                                    scale_tril=torch.diag_embed(torch.exp(0.5*log_var)))\n",
    "        z = latent.rsample()\n",
    "           \n",
    "    #    return self.decoder(z), mu, log_var\n",
    "        return self.decoder(z), latent\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(recon_x, x, mu, log_var):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_function_dist(recon_x, x, latent, input_dim):\n",
    "        prior = MultivariateNormal(loc = torch.zeros(latent.mean.shape[1]),\n",
    "                                   scale_tril=torch.eye(latent.mean.shape[1]))\n",
    "        \n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = torch.sum(kl_divergence(latent, prior))\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888f9cb-5a1a-487a-a341-d603d6a8bef3",
   "metadata": {},
   "source": [
    "VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c02e248-d6bb-4137-9fe7-72bb541f8cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        # Every data instance \n",
    "        data = data.to(DEVICE)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        recon_batch, latent = vae(data)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = VAE.loss_function_dist(recon_batch, data, latent, input_dim)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100.0 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d62557b-d245-40d7-88d0-6b804fef91c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 102268.02375\n",
      "  batch 200 loss: 94172.823671875\n",
      "LOSS train 94172.823671875 valid 107453.6328125\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 91474.396328125\n",
      "  batch 200 loss: 90896.451015625\n",
      "LOSS train 90896.451015625 valid 101955.5625\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 90095.656953125\n",
      "  batch 200 loss: 90095.94796875\n",
      "LOSS train 90095.94796875 valid 97991.1875\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 89608.863828125\n",
      "  batch 200 loss: 89559.135703125\n",
      "LOSS train 89559.135703125 valid 95764.1875\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 89406.8259375\n",
      "  batch 200 loss: 89227.239765625\n",
      "LOSS train 89227.239765625 valid 94891.2578125\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 89022.93625\n",
      "  batch 200 loss: 89222.17609375\n",
      "LOSS train 89222.17609375 valid 96044.8984375\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 89043.665078125\n",
      "  batch 200 loss: 88941.221640625\n",
      "LOSS train 88941.221640625 valid 95405.5625\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 88708.440546875\n",
      "  batch 200 loss: 88863.93671875\n",
      "LOSS train 88863.93671875 valid 95216.6796875\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 88784.285390625\n",
      "  batch 200 loss: 88682.6834375\n",
      "LOSS train 88682.6834375 valid 94786.796875\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 88591.527421875\n",
      "  batch 200 loss: 88606.34453125\n",
      "LOSS train 88606.34453125 valid 94677.9375\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 88546.216640625\n",
      "  batch 200 loss: 88471.6975\n",
      "LOSS train 88471.6975 valid 95495.8125\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 88581.218203125\n",
      "  batch 200 loss: 88342.65765625\n",
      "LOSS train 88342.65765625 valid 95037.40625\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 88296.952109375\n",
      "  batch 200 loss: 88372.99609375\n",
      "LOSS train 88372.99609375 valid 94411.1953125\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 88235.60265625\n",
      "  batch 200 loss: 88359.819140625\n",
      "LOSS train 88359.819140625 valid 96003.3125\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 88336.336875\n",
      "  batch 200 loss: 88191.4128125\n",
      "LOSS train 88191.4128125 valid 96944.453125\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 88343.865234375\n",
      "  batch 200 loss: 88093.81015625\n",
      "LOSS train 88093.81015625 valid 96887.5234375\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 88327.192109375\n",
      "  batch 200 loss: 88140.77265625\n",
      "LOSS train 88140.77265625 valid 96859.6015625\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 88329.650234375\n",
      "  batch 200 loss: 87970.19921875\n",
      "LOSS train 87970.19921875 valid 96175.8984375\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 88174.15109375\n",
      "  batch 200 loss: 87979.31625\n",
      "LOSS train 87979.31625 valid 97162.8125\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 88143.3471875\n",
      "  batch 200 loss: 88090.0615625\n",
      "LOSS train 88090.0615625 valid 97815.875\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "input_dim=rnaseq_df.shape[1]\n",
    "#vae = VAE(input_dim=input_dim, hidden_dim=[100,100], z_dim=100)\n",
    "vae = VAE(input_dim=input_dim, hidden_dim=[100], z_dim=100)\n",
    "# if torch.backends.mps.is_available():\n",
    "#     DEVICE = 'mps'\n",
    "# else:\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(torch.randn(30, 5000)), batch_size=100, shuffle=True)\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "    \n",
    "vae.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/tcga_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    vae.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    vae.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs = vdata\n",
    "        voutputs, latent = vae(vinputs)\n",
    "        \n",
    "        vloss = VAE.loss_function_dist(voutputs, vinputs, latent, input_dim)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        #torch.save(vae.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8752fd-73ca-4920-84e7-7279a81909f5",
   "metadata": {},
   "source": [
    "Shuffle the colums of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02ee5c2-0a73-4272-b686-ba5cffc6f127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe0775-60c6-404b-b898-5314d4181cbb",
   "metadata": {},
   "source": [
    "BioBomb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7746854-afd3-4580-847b-3bbd8f3b6d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tcga_biobomb_file_location = 'data/rescaled_5000_gtex_df_sort.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef961de-bdce-4cfb-b7ca-a7d352072472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tcga_df = pd.read_table(tcga_biobomb_file_location)\n",
    "tcga_df.drop(columns=tcga_df.columns[0], axis=1,  inplace=True)\n",
    "tcga_df = tcga_df.dropna()\n",
    "print(tcga_df.shape)\n",
    "tcga_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ce751a20-0ef6-4aec-93f9-600d2a85ce34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set_percent = 0.1\n",
    "tcga_df_test = rnaseq_df.sample(frac=test_set_percent)\n",
    "tcga_df_train = rnaseq_df.drop(tcga_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "472efa65-1e66-4c00-b896-769d03fea2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_tcga = CustomDataset(tcga_df_train)\n",
    "test_dataset_tcga = CustomDataset(tcga_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "59e490fd-2269-40e6-9d4f-9102f72b4962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader_tcga = torch.utils.data.DataLoader(dataset=train_dataset_tcga, batch_size=100, shuffle=True)\n",
    "validation_loader_tcga = torch.utils.data.DataLoader(dataset=test_dataset_tcga, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f01f89dc-56f1-43fa-aef2-a5e008b2ec79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 2978.0325\n",
      "LOSS train 2978.0325 valid 3333.0634765625\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2968.221875\n",
      "LOSS train 2968.221875 valid 3333.6884765625\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2990.526875\n",
      "LOSS train 2990.526875 valid 3332.948974609375\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3003.7159375\n",
      "LOSS train 3003.7159375 valid 3333.268798828125\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2992.02\n",
      "LOSS train 2992.02 valid 3332.671142578125\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2973.1271875\n",
      "LOSS train 2973.1271875 valid 3333.08740234375\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3001.1975\n",
      "LOSS train 3001.1975 valid 3333.07958984375\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3001.526875\n",
      "LOSS train 3001.526875 valid 3333.5986328125\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2986.7059375\n",
      "LOSS train 2986.7059375 valid 3332.742919921875\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2984.953125\n",
      "LOSS train 2984.953125 valid 3332.91748046875\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2986.73125\n",
      "LOSS train 2986.73125 valid 3332.461669921875\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 2983.706875\n",
      "LOSS train 2983.706875 valid 3332.802734375\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 2992.0834375\n",
      "LOSS train 2992.0834375 valid 3332.91552734375\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 2976.0678125\n",
      "LOSS train 2976.0678125 valid 3332.725830078125\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 2986.32\n",
      "LOSS train 2986.32 valid 3333.876953125\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 2968.7240625\n",
      "LOSS train 2968.7240625 valid 3332.671142578125\n",
      "EPOCH 17:\n",
      "  batch 1 loss: 2970.203125\n",
      "LOSS train 2970.203125 valid 3332.928955078125\n",
      "EPOCH 18:\n",
      "  batch 1 loss: 2995.011875\n",
      "LOSS train 2995.011875 valid 3332.9697265625\n",
      "EPOCH 19:\n",
      "  batch 1 loss: 2996.0065625\n",
      "LOSS train 2996.0065625 valid 3332.9052734375\n",
      "EPOCH 20:\n",
      "  batch 1 loss: 2985.3703125\n",
      "LOSS train 2985.3703125 valid 3332.1865234375\n",
      "EPOCH 21:\n",
      "  batch 1 loss: 2992.1021875\n",
      "LOSS train 2992.1021875 valid 3332.783203125\n",
      "EPOCH 22:\n",
      "  batch 1 loss: 2970.4903125\n",
      "LOSS train 2970.4903125 valid 3332.625244140625\n",
      "EPOCH 23:\n",
      "  batch 1 loss: 2995.64\n",
      "LOSS train 2995.64 valid 3332.918701171875\n",
      "EPOCH 24:\n",
      "  batch 1 loss: 2976.3071875\n",
      "LOSS train 2976.3071875 valid 3332.898681640625\n",
      "EPOCH 25:\n",
      "  batch 1 loss: 2968.76625\n",
      "LOSS train 2968.76625 valid 3332.99365234375\n",
      "EPOCH 26:\n",
      "  batch 1 loss: 3000.2446875\n",
      "LOSS train 3000.2446875 valid 3332.52783203125\n",
      "EPOCH 27:\n",
      "  batch 1 loss: 3009.93875\n",
      "LOSS train 3009.93875 valid 3333.301513671875\n",
      "EPOCH 28:\n",
      "  batch 1 loss: 2984.7284375\n",
      "LOSS train 2984.7284375 valid 3332.59814453125\n",
      "EPOCH 29:\n",
      "  batch 1 loss: 2977.5846875\n",
      "LOSS train 2977.5846875 valid 3333.01318359375\n",
      "EPOCH 30:\n",
      "  batch 1 loss: 2973.6165625\n",
      "LOSS train 2973.6165625 valid 3333.040283203125\n",
      "EPOCH 31:\n",
      "  batch 1 loss: 2996.786875\n",
      "LOSS train 2996.786875 valid 3332.997802734375\n",
      "EPOCH 32:\n",
      "  batch 1 loss: 2984.705625\n",
      "LOSS train 2984.705625 valid 3333.611328125\n",
      "EPOCH 33:\n",
      "  batch 1 loss: 2986.0725\n",
      "LOSS train 2986.0725 valid 3332.51220703125\n",
      "EPOCH 34:\n",
      "  batch 1 loss: 2997.01\n",
      "LOSS train 2997.01 valid 3333.271484375\n",
      "EPOCH 35:\n",
      "  batch 1 loss: 3002.58875\n",
      "LOSS train 3002.58875 valid 3333.250244140625\n",
      "EPOCH 36:\n",
      "  batch 1 loss: 2985.8309375\n",
      "LOSS train 2985.8309375 valid 3333.25537109375\n",
      "EPOCH 37:\n",
      "  batch 1 loss: 2985.445625\n",
      "LOSS train 2985.445625 valid 3332.26123046875\n",
      "EPOCH 38:\n",
      "  batch 1 loss: 2988.17375\n",
      "LOSS train 2988.17375 valid 3332.882568359375\n",
      "EPOCH 39:\n",
      "  batch 1 loss: 2976.8875\n",
      "LOSS train 2976.8875 valid 3333.593017578125\n",
      "EPOCH 40:\n",
      "  batch 1 loss: 2986.781875\n",
      "LOSS train 2986.781875 valid 3333.338134765625\n",
      "EPOCH 41:\n",
      "  batch 1 loss: 2962.0746875\n",
      "LOSS train 2962.0746875 valid 3332.873046875\n",
      "EPOCH 42:\n",
      "  batch 1 loss: 2988.3690625\n",
      "LOSS train 2988.3690625 valid 3333.1171875\n",
      "EPOCH 43:\n",
      "  batch 1 loss: 2985.3890625\n",
      "LOSS train 2985.3890625 valid 3333.09814453125\n",
      "EPOCH 44:\n",
      "  batch 1 loss: 2987.45125\n",
      "LOSS train 2987.45125 valid 3332.8720703125\n",
      "EPOCH 45:\n",
      "  batch 1 loss: 3009.2225\n",
      "LOSS train 3009.2225 valid 3332.50634765625\n",
      "EPOCH 46:\n",
      "  batch 1 loss: 2997.508125\n",
      "LOSS train 2997.508125 valid 3333.42626953125\n",
      "EPOCH 47:\n",
      "  batch 1 loss: 2976.890625\n",
      "LOSS train 2976.890625 valid 3332.835693359375\n",
      "EPOCH 48:\n",
      "  batch 1 loss: 2955.0503125\n",
      "LOSS train 2955.0503125 valid 3332.9814453125\n",
      "EPOCH 49:\n",
      "  batch 1 loss: 2979.46375\n",
      "LOSS train 2979.46375 valid 3332.9970703125\n",
      "EPOCH 50:\n",
      "  batch 1 loss: 2993.670625\n",
      "LOSS train 2993.670625 valid 3332.748046875\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "input_dim=rnaseq_df.shape[1]\n",
    "\n",
    "vae_tcga = VAE(input_dim=input_dim, hidden_dim=[100], z_dim=100)\n",
    "\n",
    "# if torch.backends.mps.is_available():\n",
    "#     DEVICE = 'mps'\n",
    "# else:\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(torch.randn(30, 5000)), batch_size=100, shuffle=True)\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "    \n",
    "vae_tcga.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(vae_tcga.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/tcga_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    vae_tcga.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    vae_tcga.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs = vdata\n",
    "        voutputs, latent = vae_tcga(vinputs)\n",
    "        \n",
    "        vloss = VAE.loss_function_dist(voutputs, vinputs, latent, input_dim)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vloss = avg_vloss/100\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        #torch.save(vae.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45ddad90-18b6-4db8-b6c0-3014f5683689",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f473677-ec3b-43d5-8432-99fcb6ddd5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
