{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44faa5b-c846-4515-b33d-e9e991325f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions.kl import kl_divergence\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import VAE_tybalt\n",
    "from VAE_tybalt import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1230036-7174-4124-aec5-411beddd9e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is normalized TCGA data from Tybalt github\n",
    "tcga_tybalt_file_location = \"data/pancan_scaled_zeroone_rnaseq.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de69b61d-e229-4898-949e-f2d6ecd833b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10459, 5000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.678296</td>\n",
       "      <td>0.289910</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.746797</td>\n",
       "      <td>0.687833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440610</td>\n",
       "      <td>0.428782</td>\n",
       "      <td>0.732819</td>\n",
       "      <td>0.634340</td>\n",
       "      <td>0.580662</td>\n",
       "      <td>0.294313</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>0.478219</td>\n",
       "      <td>0.168263</td>\n",
       "      <td>0.638497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200633</td>\n",
       "      <td>0.654917</td>\n",
       "      <td>0.181993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100606</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.092586</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.140642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620658</td>\n",
       "      <td>0.363207</td>\n",
       "      <td>0.592269</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.610192</td>\n",
       "      <td>0.374569</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.271356</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.602560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RPS4Y1      XIST      KRT5  AGR2  CEACAM5     KRT6A     KRT14   CEACAM6  \\\n",
       "0  0.678296  0.289910  0.034230   0.0      0.0  0.084731  0.031863  0.037709   \n",
       "1  0.200633  0.654917  0.181993   0.0      0.0  0.100606  0.050011  0.092586   \n",
       "\n",
       "      DDX3Y     KDM5D  ...   FAM129A   C8orf48    CDK5R1    FAM81A  C13orf18  \\\n",
       "0  0.746797  0.687833  ...  0.440610  0.428782  0.732819  0.634340  0.580662   \n",
       "1  0.103725  0.140642  ...  0.620658  0.363207  0.592269  0.602755  0.610192   \n",
       "\n",
       "      GDPD3     SMAGP   C2orf85   POU5F1B     CHST2  \n",
       "0  0.294313  0.458134  0.478219  0.168263  0.638497  \n",
       "1  0.374569  0.722420  0.271356  0.160465  0.602560  \n",
       "\n",
       "[2 rows x 5000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcga_rnaseq_df = pd.read_table(tcga_tybalt_file_location)\n",
    "tcga_rnaseq_df.drop(columns=tcga_rnaseq_df.columns[0], axis=1, inplace=True)\n",
    "tcga_rnaseq_df = tcga_rnaseq_df.dropna()\n",
    "print(tcga_rnaseq_df.shape)\n",
    "tcga_rnaseq_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f061caa7-40da-4806-9655-a42631d6c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to add gussian noise with different variance to each element of a pandas dataframe\n",
    "def add_gaussian_noise(df, variances):\n",
    "    assert len(variances) == len(\n",
    "        df.columns\n",
    "    ), \"Number of variances must match the number of columns in the DataFrame.\"\n",
    "\n",
    "    # Create a DataFrame with the same shape as the input DataFrame, filled with Gaussian random noise\n",
    "    noise = pd.DataFrame(\n",
    "        np.random.normal(0, np.sqrt(variances), size=df.shape), columns=df.columns\n",
    "    )\n",
    "\n",
    "    # Add the noise to the input DataFrame\n",
    "    noisy_df = df + noise\n",
    "\n",
    "    return noisy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8779247d-457c-4bc3-bb54-b979528b622e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variances = np.linspace(0, 0.2, len(tcga_rnaseq_df.columns))\n",
    "np.random.shuffle(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf36463-e11a-4962-850f-4ed91f5d9d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noisy_tcga_rnaseq_df = add_gaussian_noise(tcga_rnaseq_df, variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c233562-568c-45a3-8fee-71c1de995a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.002900</td>\n",
       "      <td>0.076540</td>\n",
       "      <td>0.026738</td>\n",
       "      <td>-0.187752</td>\n",
       "      <td>0.170346</td>\n",
       "      <td>0.140029</td>\n",
       "      <td>0.103744</td>\n",
       "      <td>0.343436</td>\n",
       "      <td>0.796252</td>\n",
       "      <td>0.584724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588091</td>\n",
       "      <td>0.499679</td>\n",
       "      <td>0.795139</td>\n",
       "      <td>0.476082</td>\n",
       "      <td>0.332138</td>\n",
       "      <td>0.445158</td>\n",
       "      <td>0.753875</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>-0.218744</td>\n",
       "      <td>0.820836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.208271</td>\n",
       "      <td>0.493415</td>\n",
       "      <td>-0.511877</td>\n",
       "      <td>0.207620</td>\n",
       "      <td>0.167754</td>\n",
       "      <td>0.430211</td>\n",
       "      <td>0.020135</td>\n",
       "      <td>0.836289</td>\n",
       "      <td>0.079512</td>\n",
       "      <td>0.505806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734051</td>\n",
       "      <td>0.601302</td>\n",
       "      <td>0.720078</td>\n",
       "      <td>0.361085</td>\n",
       "      <td>0.284556</td>\n",
       "      <td>0.509014</td>\n",
       "      <td>0.426944</td>\n",
       "      <td>1.055141</td>\n",
       "      <td>-0.306562</td>\n",
       "      <td>0.339480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.042233</td>\n",
       "      <td>0.569753</td>\n",
       "      <td>-0.194156</td>\n",
       "      <td>0.244873</td>\n",
       "      <td>-0.442123</td>\n",
       "      <td>0.126243</td>\n",
       "      <td>0.037735</td>\n",
       "      <td>-0.700100</td>\n",
       "      <td>0.655012</td>\n",
       "      <td>0.422450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308693</td>\n",
       "      <td>0.752764</td>\n",
       "      <td>0.684626</td>\n",
       "      <td>0.035813</td>\n",
       "      <td>0.203002</td>\n",
       "      <td>0.484434</td>\n",
       "      <td>0.346695</td>\n",
       "      <td>0.665758</td>\n",
       "      <td>0.403595</td>\n",
       "      <td>0.085154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.706050</td>\n",
       "      <td>0.354596</td>\n",
       "      <td>0.181197</td>\n",
       "      <td>0.149144</td>\n",
       "      <td>-0.183885</td>\n",
       "      <td>-0.063603</td>\n",
       "      <td>-0.007540</td>\n",
       "      <td>-0.912117</td>\n",
       "      <td>0.477399</td>\n",
       "      <td>1.005240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462627</td>\n",
       "      <td>0.419944</td>\n",
       "      <td>0.932775</td>\n",
       "      <td>1.262112</td>\n",
       "      <td>0.982610</td>\n",
       "      <td>0.533770</td>\n",
       "      <td>0.729427</td>\n",
       "      <td>-0.173217</td>\n",
       "      <td>0.207326</td>\n",
       "      <td>0.202362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.787770</td>\n",
       "      <td>0.017812</td>\n",
       "      <td>0.044124</td>\n",
       "      <td>-0.216968</td>\n",
       "      <td>-0.292206</td>\n",
       "      <td>-0.270384</td>\n",
       "      <td>0.115107</td>\n",
       "      <td>-0.120718</td>\n",
       "      <td>0.690189</td>\n",
       "      <td>0.949843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583401</td>\n",
       "      <td>0.166573</td>\n",
       "      <td>0.827972</td>\n",
       "      <td>0.216888</td>\n",
       "      <td>-0.244023</td>\n",
       "      <td>0.050730</td>\n",
       "      <td>-0.137345</td>\n",
       "      <td>0.165989</td>\n",
       "      <td>0.136734</td>\n",
       "      <td>1.402816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RPS4Y1      XIST      KRT5      AGR2   CEACAM5     KRT6A     KRT14  \\\n",
       "0  1.002900  0.076540  0.026738 -0.187752  0.170346  0.140029  0.103744   \n",
       "1  0.208271  0.493415 -0.511877  0.207620  0.167754  0.430211  0.020135   \n",
       "2  1.042233  0.569753 -0.194156  0.244873 -0.442123  0.126243  0.037735   \n",
       "3  0.706050  0.354596  0.181197  0.149144 -0.183885 -0.063603 -0.007540   \n",
       "4  0.787770  0.017812  0.044124 -0.216968 -0.292206 -0.270384  0.115107   \n",
       "\n",
       "    CEACAM6     DDX3Y     KDM5D  ...   FAM129A   C8orf48    CDK5R1    FAM81A  \\\n",
       "0  0.343436  0.796252  0.584724  ...  0.588091  0.499679  0.795139  0.476082   \n",
       "1  0.836289  0.079512  0.505806  ...  0.734051  0.601302  0.720078  0.361085   \n",
       "2 -0.700100  0.655012  0.422450  ...  0.308693  0.752764  0.684626  0.035813   \n",
       "3 -0.912117  0.477399  1.005240  ...  0.462627  0.419944  0.932775  1.262112   \n",
       "4 -0.120718  0.690189  0.949843  ...  0.583401  0.166573  0.827972  0.216888   \n",
       "\n",
       "   C13orf18     GDPD3     SMAGP   C2orf85   POU5F1B     CHST2  \n",
       "0  0.332138  0.445158  0.753875  0.026667 -0.218744  0.820836  \n",
       "1  0.284556  0.509014  0.426944  1.055141 -0.306562  0.339480  \n",
       "2  0.203002  0.484434  0.346695  0.665758  0.403595  0.085154  \n",
       "3  0.982610  0.533770  0.729427 -0.173217  0.207326  0.202362  \n",
       "4 -0.244023  0.050730 -0.137345  0.165989  0.136734  1.402816  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_tcga_rnaseq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c385efd9-0ca2-4854-a307-1c2bc572ee7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.678296</td>\n",
       "      <td>0.289910</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.746797</td>\n",
       "      <td>0.687833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440610</td>\n",
       "      <td>0.428782</td>\n",
       "      <td>0.732819</td>\n",
       "      <td>0.634340</td>\n",
       "      <td>0.580662</td>\n",
       "      <td>0.294313</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>0.478219</td>\n",
       "      <td>0.168263</td>\n",
       "      <td>0.638497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200633</td>\n",
       "      <td>0.654917</td>\n",
       "      <td>0.181993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100606</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.092586</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.140642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620658</td>\n",
       "      <td>0.363207</td>\n",
       "      <td>0.592269</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.610192</td>\n",
       "      <td>0.374569</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.271356</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.602560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.785980</td>\n",
       "      <td>0.140842</td>\n",
       "      <td>0.081082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730648</td>\n",
       "      <td>0.657189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437658</td>\n",
       "      <td>0.471489</td>\n",
       "      <td>0.868774</td>\n",
       "      <td>0.471141</td>\n",
       "      <td>0.487212</td>\n",
       "      <td>0.385521</td>\n",
       "      <td>0.466642</td>\n",
       "      <td>0.784059</td>\n",
       "      <td>0.160797</td>\n",
       "      <td>0.557074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RPS4Y1      XIST      KRT5  AGR2  CEACAM5     KRT6A     KRT14   CEACAM6  \\\n",
       "0  0.678296  0.289910  0.034230   0.0      0.0  0.084731  0.031863  0.037709   \n",
       "1  0.200633  0.654917  0.181993   0.0      0.0  0.100606  0.050011  0.092586   \n",
       "2  0.785980  0.140842  0.081082   0.0      0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "      DDX3Y     KDM5D  ...   FAM129A   C8orf48    CDK5R1    FAM81A  C13orf18  \\\n",
       "0  0.746797  0.687833  ...  0.440610  0.428782  0.732819  0.634340  0.580662   \n",
       "1  0.103725  0.140642  ...  0.620658  0.363207  0.592269  0.602755  0.610192   \n",
       "2  0.730648  0.657189  ...  0.437658  0.471489  0.868774  0.471141  0.487212   \n",
       "\n",
       "      GDPD3     SMAGP   C2orf85   POU5F1B     CHST2  \n",
       "0  0.294313  0.458134  0.478219  0.168263  0.638497  \n",
       "1  0.374569  0.722420  0.271356  0.160465  0.602560  \n",
       "2  0.385521  0.466642  0.784059  0.160797  0.557074  \n",
       "\n",
       "[3 rows x 5000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcga_rnaseq_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5e174-c4c8-4540-b202-fd4264f5ba82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77574cb6-49d0-4b65-8990-3d463bc57e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set_percent = 0.2\n",
    "tcga_df_test = tcga_rnaseq_df.sample(frac=test_set_percent)\n",
    "tcga_df_train = tcga_rnaseq_df.drop(tcga_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b29ed0-a7b1-4007-8ed1-edf2dffdb2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set_percent = 0.2\n",
    "noisy_tcga_df_test = noisy_tcga_rnaseq_df.sample(frac=test_set_percent)\n",
    "noisy_tcga_df_train = noisy_tcga_rnaseq_df.drop(noisy_tcga_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d8cce9-1dfe-4393-bbbd-8aa528eeda6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e28fab9-7379-4f64-a5b2-9ec2634b1898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(tcga_df_train)\n",
    "test_dataset = CustomDataset(tcga_df_test)\n",
    "noisy_train_dataset = CustomDataset(noisy_tcga_df_train)\n",
    "noisy_test_dataset = CustomDataset(noisy_tcga_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "218cf786-0355-44b5-b6b2-46a2b7bdb45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=32, shuffle=False\n",
    ")\n",
    "noisy_train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=noisy_train_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "noisy_validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset=noisy_test_dataset, batch_size=32, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e244b16-2be7-46df-837b-f4ea845b4ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer, train_loader):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance\n",
    "        data = data.to(DEVICE)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        recon_batch, latent = model(data)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = VAE.loss_function_dist(recon_batch, data, latent, input_dim)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100.0  # loss per batch\n",
    "            print(\"  batch {} loss: {}\".format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar(\"Loss/train\", last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58f31080-0141-4cf2-9796-744d88782289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf runs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac815b8d-1d0a-4493-b7c5-35849c9130ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 96026.9003125\n",
      "  batch 200 loss: 91149.7521875\n",
      "LOSS train 91149.7521875 valid 100409.96875\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 90055.60265625\n",
      "  batch 200 loss: 89582.620546875\n",
      "LOSS train 89582.620546875 valid 94886.1875\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 89173.324140625\n",
      "  batch 200 loss: 88848.005078125\n",
      "LOSS train 88848.005078125 valid 94675.2578125\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 88708.456796875\n",
      "  batch 200 loss: 88551.723359375\n",
      "LOSS train 88551.723359375 valid 93217.359375\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 88349.42546875\n",
      "  batch 200 loss: 88439.39890625\n",
      "LOSS train 88439.39890625 valid 94100.8125\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 88335.243984375\n",
      "  batch 200 loss: 88206.891875\n",
      "LOSS train 88206.891875 valid 93082.2890625\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 88062.640703125\n",
      "  batch 200 loss: 88132.57484375\n",
      "LOSS train 88132.57484375 valid 92943.3984375\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 87861.62875\n",
      "  batch 200 loss: 88076.188203125\n",
      "LOSS train 88076.188203125 valid 93667.171875\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 87952.724609375\n",
      "  batch 200 loss: 87866.784921875\n",
      "LOSS train 87866.784921875 valid 93070.1640625\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 87938.5146875\n",
      "  batch 200 loss: 87663.823125\n",
      "LOSS train 87663.823125 valid 93057.171875\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 87781.636015625\n",
      "  batch 200 loss: 87598.664375\n",
      "LOSS train 87598.664375 valid 93346.734375\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 87719.22546875\n",
      "  batch 200 loss: 87688.229140625\n",
      "LOSS train 87688.229140625 valid 92483.3125\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 87639.25765625\n",
      "  batch 200 loss: 87782.7746875\n",
      "LOSS train 87782.7746875 valid 93239.734375\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 87485.30671875\n",
      "  batch 200 loss: 87684.52\n",
      "LOSS train 87684.52 valid 93570.703125\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 87624.6928125\n",
      "  batch 200 loss: 87572.479140625\n",
      "LOSS train 87572.479140625 valid 92954.5625\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 87436.162421875\n",
      "  batch 200 loss: 87555.244765625\n",
      "LOSS train 87555.244765625 valid 93608.90625\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 87379.77859375\n",
      "  batch 200 loss: 87552.215625\n",
      "LOSS train 87552.215625 valid 92533.484375\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 87468.65609375\n",
      "  batch 200 loss: 87431.718515625\n",
      "LOSS train 87431.718515625 valid 94943.2421875\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 87337.1040625\n",
      "  batch 200 loss: 87626.7803125\n",
      "LOSS train 87626.7803125 valid 92191.375\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 87311.7075\n",
      "  batch 200 loss: 87348.4825\n",
      "LOSS train 87348.4825 valid 93448.515625\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 87416.745546875\n",
      "  batch 200 loss: 87431.823203125\n",
      "LOSS train 87431.823203125 valid 93391.859375\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 87387.35296875\n",
      "  batch 200 loss: 87360.76296875\n",
      "LOSS train 87360.76296875 valid 93419.734375\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 87371.4471875\n",
      "  batch 200 loss: 87320.9321875\n",
      "LOSS train 87320.9321875 valid 92493.7265625\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 87429.026640625\n",
      "  batch 200 loss: 87235.0859375\n",
      "LOSS train 87235.0859375 valid 93405.9375\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 87382.412421875\n",
      "  batch 200 loss: 87244.25078125\n",
      "LOSS train 87244.25078125 valid 93627.3203125\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 87272.67890625\n",
      "  batch 200 loss: 87298.46734375\n",
      "LOSS train 87298.46734375 valid 93236.484375\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 87095.7709375\n",
      "  batch 200 loss: 87355.612578125\n",
      "LOSS train 87355.612578125 valid 92996.4375\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 87391.43234375\n",
      "  batch 200 loss: 87146.396328125\n",
      "LOSS train 87146.396328125 valid 93752.8125\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 87208.13421875\n",
      "  batch 200 loss: 87222.016328125\n",
      "LOSS train 87222.016328125 valid 92463.234375\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 87199.536171875\n",
      "  batch 200 loss: 87318.795078125\n",
      "LOSS train 87318.795078125 valid 93379.1015625\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 87065.7546875\n",
      "  batch 200 loss: 87184.894453125\n",
      "LOSS train 87184.894453125 valid 92976.4609375\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 87155.81734375\n",
      "  batch 200 loss: 87327.17703125\n",
      "LOSS train 87327.17703125 valid 92198.34375\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 87129.787265625\n",
      "  batch 200 loss: 87308.048828125\n",
      "LOSS train 87308.048828125 valid 92826.328125\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 87031.23234375\n",
      "  batch 200 loss: 87224.7684375\n",
      "LOSS train 87224.7684375 valid 93235.3984375\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 87167.036015625\n",
      "  batch 200 loss: 87167.7321875\n",
      "LOSS train 87167.7321875 valid 93356.578125\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 87150.954140625\n",
      "  batch 200 loss: 87073.966328125\n",
      "LOSS train 87073.966328125 valid 93323.5703125\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 87078.30015625\n",
      "  batch 200 loss: 87229.277265625\n",
      "LOSS train 87229.277265625 valid 93218.375\n",
      "EPOCH 38:\n",
      "  batch 100 loss: 87061.765703125\n",
      "  batch 200 loss: 87226.307421875\n",
      "LOSS train 87226.307421875 valid 93800.734375\n",
      "EPOCH 39:\n",
      "  batch 100 loss: 87033.872734375\n",
      "  batch 200 loss: 87139.87640625\n",
      "LOSS train 87139.87640625 valid 93550.8984375\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 87135.034921875\n",
      "  batch 200 loss: 86980.11109375\n",
      "LOSS train 86980.11109375 valid 94511.3671875\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "input_dim = tcga_rnaseq_df.shape[1]\n",
    "# vae = VAE(input_dim=input_dim, hidden_dim=[100,100], z_dim=100)\n",
    "\n",
    "model = VAE(input_dim=5000, hidden_dim=[512, 256], z_dim=100)\n",
    "# if torch.backends.mps.is_available():\n",
    "#     DEVICE = 'mps'\n",
    "# else:\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(torch.randn(30, 5000)), batch_size=100, shuffle=True)\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(\"runs/tcga_trainer_{}\".format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "best_vloss = 1_000_000.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}:\".format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer, train_loader)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs = vdata\n",
    "        voutputs, latent = model(vinputs)\n",
    "\n",
    "        vloss = VAE.loss_function_dist(voutputs, vinputs, latent, input_dim)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(\"LOSS train {} valid {}\".format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars(\n",
    "        \"Training vs. Validation Loss\",\n",
    "        {\"Training\": avg_loss, \"Validation\": avg_vloss},\n",
    "        epoch_number + 1,\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = \"model_{}_{}\".format(timestamp, epoch_number)\n",
    "        # torch.save(vae.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ee2d-8b2c-4e53-bea7-c8d80f704232",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Look at learning with noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850cf2bf-e3b3-4893-b109-32296a166304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 96568.7828125\n",
      "  batch 200 loss: 92017.350078125\n",
      "LOSS train 92017.350078125 valid 99313.8515625\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 90339.232734375\n",
      "  batch 200 loss: 90166.429921875\n",
      "LOSS train 90166.429921875 valid 94253.2109375\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 89617.91296875\n",
      "  batch 200 loss: 89359.778125\n",
      "LOSS train 89359.778125 valid 93638.671875\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 89011.6009375\n",
      "  batch 200 loss: 89024.31328125\n",
      "LOSS train 89024.31328125 valid 93340.5625\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 88667.4715625\n",
      "  batch 200 loss: 88737.743515625\n",
      "LOSS train 88737.743515625 valid 93178.9375\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 88371.89921875\n",
      "  batch 200 loss: 88544.034140625\n",
      "LOSS train 88544.034140625 valid 92935.25\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 88469.122890625\n",
      "  batch 200 loss: 88161.65546875\n",
      "LOSS train 88161.65546875 valid 92567.1796875\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 88014.2909375\n",
      "  batch 200 loss: 88287.259765625\n",
      "LOSS train 88287.259765625 valid 92807.734375\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 87938.80046875\n",
      "  batch 200 loss: 87943.30703125\n",
      "LOSS train 87943.30703125 valid 92873.5703125\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 87901.25671875\n",
      "  batch 200 loss: 87872.7446875\n",
      "LOSS train 87872.7446875 valid 92670.1328125\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 87739.206640625\n",
      "  batch 200 loss: 87883.004609375\n",
      "LOSS train 87883.004609375 valid 92848.546875\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 87703.889765625\n",
      "  batch 200 loss: 87766.987421875\n",
      "LOSS train 87766.987421875 valid 92488.1640625\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 87575.5253125\n",
      "  batch 200 loss: 87476.237734375\n",
      "LOSS train 87476.237734375 valid 92372.6875\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 87563.2775\n",
      "  batch 200 loss: 87609.25453125\n",
      "LOSS train 87609.25453125 valid 91679.125\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 87195.72625\n",
      "  batch 200 loss: 87671.028828125\n",
      "LOSS train 87671.028828125 valid 92410.9921875\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 87398.605390625\n",
      "  batch 200 loss: 87482.061953125\n",
      "LOSS train 87482.061953125 valid 92206.0625\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 87258.339765625\n",
      "  batch 200 loss: 87286.734375\n",
      "LOSS train 87286.734375 valid 92199.609375\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 87248.94890625\n",
      "  batch 200 loss: 87322.5946875\n",
      "LOSS train 87322.5946875 valid 92327.046875\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 87342.206640625\n",
      "  batch 200 loss: 87201.883984375\n",
      "LOSS train 87201.883984375 valid 92092.3125\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 87077.5496875\n",
      "  batch 200 loss: 87320.266640625\n",
      "LOSS train 87320.266640625 valid 91784.359375\n",
      "EPOCH 21:\n",
      "  batch 100 loss: 87058.2715625\n",
      "  batch 200 loss: 87340.0265625\n",
      "LOSS train 87340.0265625 valid 92066.046875\n",
      "EPOCH 22:\n",
      "  batch 100 loss: 87154.10875\n",
      "  batch 200 loss: 87204.47078125\n",
      "LOSS train 87204.47078125 valid 91466.265625\n",
      "EPOCH 23:\n",
      "  batch 100 loss: 87088.04328125\n",
      "  batch 200 loss: 87121.973671875\n",
      "LOSS train 87121.973671875 valid 92039.7890625\n",
      "EPOCH 24:\n",
      "  batch 100 loss: 86961.61234375\n",
      "  batch 200 loss: 87109.013515625\n",
      "LOSS train 87109.013515625 valid 92519.046875\n",
      "EPOCH 25:\n",
      "  batch 100 loss: 86934.456171875\n",
      "  batch 200 loss: 86962.459375\n",
      "LOSS train 86962.459375 valid 92156.4765625\n",
      "EPOCH 26:\n",
      "  batch 100 loss: 86852.16\n",
      "  batch 200 loss: 87111.40859375\n",
      "LOSS train 87111.40859375 valid 92003.7265625\n",
      "EPOCH 27:\n",
      "  batch 100 loss: 86637.191328125\n",
      "  batch 200 loss: 86986.640390625\n",
      "LOSS train 86986.640390625 valid 91854.9453125\n",
      "EPOCH 28:\n",
      "  batch 100 loss: 86792.685\n",
      "  batch 200 loss: 86950.1753125\n",
      "LOSS train 86950.1753125 valid 92455.0234375\n",
      "EPOCH 29:\n",
      "  batch 100 loss: 86729.49453125\n",
      "  batch 200 loss: 86874.812109375\n",
      "LOSS train 86874.812109375 valid 92107.9296875\n",
      "EPOCH 30:\n",
      "  batch 100 loss: 86777.191796875\n",
      "  batch 200 loss: 86630.79484375\n",
      "LOSS train 86630.79484375 valid 91866.765625\n",
      "EPOCH 31:\n",
      "  batch 100 loss: 86570.94765625\n",
      "  batch 200 loss: 86872.946171875\n",
      "LOSS train 86872.946171875 valid 92436.2421875\n",
      "EPOCH 32:\n",
      "  batch 100 loss: 86726.36140625\n",
      "  batch 200 loss: 86718.766875\n",
      "LOSS train 86718.766875 valid 92116.7578125\n",
      "EPOCH 33:\n",
      "  batch 100 loss: 86609.474140625\n",
      "  batch 200 loss: 86558.32484375\n",
      "LOSS train 86558.32484375 valid 92075.9296875\n",
      "EPOCH 34:\n",
      "  batch 100 loss: 86466.360078125\n",
      "  batch 200 loss: 86666.4325\n",
      "LOSS train 86666.4325 valid 92689.5078125\n",
      "EPOCH 35:\n",
      "  batch 100 loss: 86501.485703125\n",
      "  batch 200 loss: 86557.631875\n",
      "LOSS train 86557.631875 valid 92587.3828125\n",
      "EPOCH 36:\n",
      "  batch 100 loss: 86434.6478125\n",
      "  batch 200 loss: 86584.861796875\n",
      "LOSS train 86584.861796875 valid 93308.3515625\n",
      "EPOCH 37:\n",
      "  batch 100 loss: 86278.273359375\n",
      "  batch 200 loss: 86748.642421875\n",
      "LOSS train 86748.642421875 valid 93491.453125\n",
      "EPOCH 38:\n",
      "  batch 100 loss: 86268.713125\n",
      "  batch 200 loss: 86643.42890625\n",
      "LOSS train 86643.42890625 valid 93136.3984375\n",
      "EPOCH 39:\n",
      "  batch 100 loss: 86296.745703125\n",
      "  batch 200 loss: 86422.01265625\n",
      "LOSS train 86422.01265625 valid 93740.796875\n",
      "EPOCH 40:\n",
      "  batch 100 loss: 86331.5228125\n",
      "  batch 200 loss: 86424.525234375\n",
      "LOSS train 86424.525234375 valid 93303.7734375\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "input_dim = tcga_rnaseq_df.shape[1]\n",
    "# vae = VAE(input_dim=input_dim, hidden_dim=[100,100], z_dim=100)\n",
    "model = VAE(input_dim=5000, hidden_dim=[512, 256], z_dim=100)\n",
    "# if torch.backends.mps.is_available():\n",
    "#     DEVICE = 'mps'\n",
    "# else:\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(torch.randn(30, 5000)), batch_size=100, shuffle=True)\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(\"runs/tcga_trainer_{}\".format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "best_vloss = 1_000_000.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}:\".format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer, noisy_train_loader)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(noisy_validation_loader):\n",
    "        vinputs = vdata\n",
    "        voutputs, latent = model(vinputs)\n",
    "\n",
    "        vloss = VAE.loss_function_dist(voutputs, vinputs, latent, input_dim)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(\"LOSS train {} valid {}\".format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars(\n",
    "        \"Training vs. Validation Loss\",\n",
    "        {\"Training\": avg_loss, \"Validation\": avg_vloss},\n",
    "        epoch_number + 1,\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = \"model_{}_{}\".format(timestamp, epoch_number)\n",
    "        # torch.save(vae.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be880b-033d-4164-a4d1-2167279b1a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
