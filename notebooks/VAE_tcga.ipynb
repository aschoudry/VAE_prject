{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088ec08c-d677-426b-aa50-e1b2007fb393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import random\n",
    "from random import sample\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61260330-bf50-45a6-ac05-0e50d304ad4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf runs/tcga*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7fb5d5e-8533-4cd3-87c4-7cce50c13d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tcga_tybalt_file_location = 'data/pancan_scaled_zeroone_rnaseq.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ee6a57f-c5ed-4524-ac85-e7b3bbd06f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10459, 5000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.678296</td>\n",
       "      <td>0.289910</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084731</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.746797</td>\n",
       "      <td>0.687833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440610</td>\n",
       "      <td>0.428782</td>\n",
       "      <td>0.732819</td>\n",
       "      <td>0.634340</td>\n",
       "      <td>0.580662</td>\n",
       "      <td>0.294313</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>0.478219</td>\n",
       "      <td>0.168263</td>\n",
       "      <td>0.638497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.200633</td>\n",
       "      <td>0.654917</td>\n",
       "      <td>0.181993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100606</td>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.092586</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.140642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620658</td>\n",
       "      <td>0.363207</td>\n",
       "      <td>0.592269</td>\n",
       "      <td>0.602755</td>\n",
       "      <td>0.610192</td>\n",
       "      <td>0.374569</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.271356</td>\n",
       "      <td>0.160465</td>\n",
       "      <td>0.602560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RPS4Y1      XIST      KRT5  AGR2  CEACAM5     KRT6A     KRT14   CEACAM6  \\\n",
       "0  0.678296  0.289910  0.034230   0.0      0.0  0.084731  0.031863  0.037709   \n",
       "1  0.200633  0.654917  0.181993   0.0      0.0  0.100606  0.050011  0.092586   \n",
       "\n",
       "      DDX3Y     KDM5D  ...   FAM129A   C8orf48    CDK5R1    FAM81A  C13orf18  \\\n",
       "0  0.746797  0.687833  ...  0.440610  0.428782  0.732819  0.634340  0.580662   \n",
       "1  0.103725  0.140642  ...  0.620658  0.363207  0.592269  0.602755  0.610192   \n",
       "\n",
       "      GDPD3     SMAGP   C2orf85   POU5F1B     CHST2  \n",
       "0  0.294313  0.458134  0.478219  0.168263  0.638497  \n",
       "1  0.374569  0.722420  0.271356  0.160465  0.602560  \n",
       "\n",
       "[2 rows x 5000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnaseq_df = pd.read_table(tcga_tybalt_file_location)\n",
    "rnaseq_df.drop(columns=rnaseq_df.columns[0], axis=1,  inplace=True)\n",
    "rnaseq_df = rnaseq_df.dropna()\n",
    "print(rnaseq_df.shape)\n",
    "rnaseq_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7afa172e-c31d-4de1-96e2-7afe6a96a8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set_percent = 0.2\n",
    "rnaseq_df_test = rnaseq_df.sample(frac=test_set_percent)\n",
    "rnaseq_df_train = rnaseq_df.drop(rnaseq_df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c7553cd-bc1e-4ea1-a026-8ffceca9d9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx].values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b88d241-c51d-447c-ab2e-35c06a1a9e82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(rnaseq_df_train)\n",
    "test_dataset = CustomDataset(rnaseq_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65571fe1-7693-4f1b-a809-8e3df56e0d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=50, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99018251-fbe3-41df-8d9d-6f28e96c05f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: list, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([nn.Linear(input_dim, hidden_dim[0])])\n",
    "        self.decoder_layers = nn.ModuleList([nn.Linear(hidden_dim[0], input_dim)])\n",
    "                \n",
    "        if len(hidden_dim)>1:\n",
    "            for i in range(len(hidden_dim)-1):\n",
    "                self.encoder_layers.append(nn.Linear(hidden_dim[i], hidden_dim[i+1]))\n",
    "                self.decoder_layers.insert(0, nn.Linear(hidden_dim[i+1], hidden_dim[i]))\n",
    "                \n",
    "        self.encoder_layers.append(nn.Linear(hidden_dim[-1], 2 * z_dim))\n",
    "        self.batchnorm = nn.BatchNorm1d(z_dim)\n",
    "        self.decoder_layers.insert(0, nn.Linear(z_dim, hidden_dim[-1]))\n",
    "\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        for idx, layer in enumerate(self.encoder_layers):\n",
    "            x = layer(x)\n",
    "            if idx < len(self.encoder_layers) - 1:\n",
    "                # x = F.dropout(x, 0.01)\n",
    "                x = F.relu(x)\n",
    "                #x = nn.BatchNorm1d(x)\n",
    "        return x[...,:self.z_dim], x[...,self.z_dim:] # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        # std = torch.abs(log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        for idx, layer in enumerate(self.decoder_layers):\n",
    "            z = layer(z)\n",
    "            if idx < len(self.decoder_layers) - 1:\n",
    "                # x = F.dropout(x, 0.01)\n",
    "                z = F.relu(z)\n",
    "        return torch.sigmoid(z) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, input_dim))\n",
    "        mu = self.batchnorm(mu)\n",
    "        log_var = self.batchnorm(log_var)\n",
    "    #    z = self.sampling(mu, log_var)\n",
    "        latent = MultivariateNormal(loc = mu, \n",
    "                                    scale_tril=torch.diag_embed(torch.exp(0.5*log_var)))\n",
    "        z = latent.rsample()\n",
    "           \n",
    "    #    return self.decoder(z), mu, log_var\n",
    "        return self.decoder(z), latent\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(recon_x, x, mu, log_var):\n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_function_dist(recon_x, x, latent, input_dim):\n",
    "        prior = MultivariateNormal(loc = torch.zeros(latent.mean.shape[1]),\n",
    "                                   scale_tril=torch.eye(latent.mean.shape[1]))\n",
    "        \n",
    "        BCE = F.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "        KLD = torch.sum(kl_divergence(latent, prior))\n",
    "        return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888f9cb-5a1a-487a-a341-d603d6a8bef3",
   "metadata": {},
   "source": [
    "VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c02e248-d6bb-4137-9fe7-72bb541f8cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        # Every data instance \n",
    "        data = data.to(DEVICE)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        recon_batch, latent = vae(data)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = VAE.loss_function_dist(recon_batch, data, latent, input_dim)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100.0 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c7fc8c8-8d2f-46e8-9a56-044ae0e3dc8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 100 loss: 159118.78109375\n",
      "LOSS train 159118.78109375 valid 207546.734375\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 143969.975\n",
      "LOSS train 143969.975 valid 187245.34375\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 141760.40515625\n",
      "LOSS train 141760.40515625 valid 172713.28125\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 140533.98828125\n",
      "LOSS train 140533.98828125 valid 173668.71875\n",
      "EPOCH 5:\n",
      "  batch 100 loss: 140028.98203125\n",
      "LOSS train 140028.98203125 valid 164026.125\n",
      "EPOCH 6:\n",
      "  batch 100 loss: 139635.4678125\n",
      "LOSS train 139635.4678125 valid 161267.953125\n",
      "EPOCH 7:\n",
      "  batch 100 loss: 139351.01046875\n",
      "LOSS train 139351.01046875 valid 160423.046875\n",
      "EPOCH 8:\n",
      "  batch 100 loss: 138997.33484375\n",
      "LOSS train 138997.33484375 valid 160612.171875\n",
      "EPOCH 9:\n",
      "  batch 100 loss: 138725.40625\n",
      "LOSS train 138725.40625 valid 157223.515625\n",
      "EPOCH 10:\n",
      "  batch 100 loss: 138685.10375\n",
      "LOSS train 138685.10375 valid 155908.421875\n",
      "EPOCH 11:\n",
      "  batch 100 loss: 138485.31078125\n",
      "LOSS train 138485.31078125 valid 154812.625\n",
      "EPOCH 12:\n",
      "  batch 100 loss: 138628.09421875\n",
      "LOSS train 138628.09421875 valid 156489.21875\n",
      "EPOCH 13:\n",
      "  batch 100 loss: 138434.93125\n",
      "LOSS train 138434.93125 valid 153315.8125\n",
      "EPOCH 14:\n",
      "  batch 100 loss: 138334.905625\n",
      "LOSS train 138334.905625 valid 152964.703125\n",
      "EPOCH 15:\n",
      "  batch 100 loss: 138131.92859375\n",
      "LOSS train 138131.92859375 valid 151393.796875\n",
      "EPOCH 16:\n",
      "  batch 100 loss: 138148.006875\n",
      "LOSS train 138148.006875 valid 155802.828125\n",
      "EPOCH 17:\n",
      "  batch 100 loss: 137864.435\n",
      "LOSS train 137864.435 valid 156312.984375\n",
      "EPOCH 18:\n",
      "  batch 100 loss: 137900.36015625\n",
      "LOSS train 137900.36015625 valid 155293.078125\n",
      "EPOCH 19:\n",
      "  batch 100 loss: 137776.46734375\n",
      "LOSS train 137776.46734375 valid 155040.5\n",
      "EPOCH 20:\n",
      "  batch 100 loss: 137784.435\n",
      "LOSS train 137784.435 valid 154956.328125\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "input_dim=rnaseq_df.shape[1]\n",
    "#vae = VAE(input_dim=input_dim, hidden_dim=[100,100], z_dim=100)\n",
    "vae = VAE(input_dim=input_dim, hidden_dim=[100], z_dim=100)\n",
    "# if torch.backends.mps.is_available():\n",
    "#     DEVICE = 'mps'\n",
    "# else:\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=torch.Tensor(torch.randn(30, 5000)), batch_size=100, shuffle=True)\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "    \n",
    "vae.to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/tcga_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    vae.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    vae.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs = vdata\n",
    "        voutputs, latent = vae(vinputs)\n",
    "        \n",
    "        vloss = VAE.loss_function_dist(voutputs, vinputs, latent, input_dim)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        #torch.save(vae.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8752fd-73ca-4920-84e7-7279a81909f5",
   "metadata": {},
   "source": [
    "Shuffle the colums of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e02ee5c2-0a73-4272-b686-ba5cffc6f127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe0775-60c6-404b-b898-5314d4181cbb",
   "metadata": {},
   "source": [
    "BioBomb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f473677-ec3b-43d5-8432-99fcb6ddd5bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnaseq_df_test.to_csv('data/rnaseq_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c6d509-1e68-405b-812f-63eeb5840061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPS4Y1</th>\n",
       "      <th>XIST</th>\n",
       "      <th>KRT5</th>\n",
       "      <th>AGR2</th>\n",
       "      <th>CEACAM5</th>\n",
       "      <th>KRT6A</th>\n",
       "      <th>KRT14</th>\n",
       "      <th>CEACAM6</th>\n",
       "      <th>DDX3Y</th>\n",
       "      <th>KDM5D</th>\n",
       "      <th>...</th>\n",
       "      <th>FAM129A</th>\n",
       "      <th>C8orf48</th>\n",
       "      <th>CDK5R1</th>\n",
       "      <th>FAM81A</th>\n",
       "      <th>C13orf18</th>\n",
       "      <th>GDPD3</th>\n",
       "      <th>SMAGP</th>\n",
       "      <th>C2orf85</th>\n",
       "      <th>POU5F1B</th>\n",
       "      <th>CHST2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676411</td>\n",
       "      <td>0.538535</td>\n",
       "      <td>0.829060</td>\n",
       "      <td>0.371617</td>\n",
       "      <td>0.076709</td>\n",
       "      <td>0.493035</td>\n",
       "      <td>0.495491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641401</td>\n",
       "      <td>0.258891</td>\n",
       "      <td>0.520563</td>\n",
       "      <td>0.400908</td>\n",
       "      <td>0.437579</td>\n",
       "      <td>0.560402</td>\n",
       "      <td>0.483713</td>\n",
       "      <td>0.054940</td>\n",
       "      <td>0.333915</td>\n",
       "      <td>0.313550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5638</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601335</td>\n",
       "      <td>0.791538</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>0.711329</td>\n",
       "      <td>0.771928</td>\n",
       "      <td>0.557838</td>\n",
       "      <td>0.656916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608653</td>\n",
       "      <td>0.157189</td>\n",
       "      <td>0.526314</td>\n",
       "      <td>0.651298</td>\n",
       "      <td>0.422983</td>\n",
       "      <td>0.648942</td>\n",
       "      <td>0.718139</td>\n",
       "      <td>0.030541</td>\n",
       "      <td>0.219439</td>\n",
       "      <td>0.447640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801651</td>\n",
       "      <td>0.515508</td>\n",
       "      <td>0.526579</td>\n",
       "      <td>0.119657</td>\n",
       "      <td>0.212524</td>\n",
       "      <td>0.086728</td>\n",
       "      <td>0.253842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278018</td>\n",
       "      <td>0.160775</td>\n",
       "      <td>0.496794</td>\n",
       "      <td>0.384108</td>\n",
       "      <td>0.434125</td>\n",
       "      <td>0.476615</td>\n",
       "      <td>0.634136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433654</td>\n",
       "      <td>0.308404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>0.767083</td>\n",
       "      <td>0.132404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046623</td>\n",
       "      <td>0.774512</td>\n",
       "      <td>0.736268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359250</td>\n",
       "      <td>0.657670</td>\n",
       "      <td>0.340666</td>\n",
       "      <td>0.708832</td>\n",
       "      <td>0.358783</td>\n",
       "      <td>0.328927</td>\n",
       "      <td>0.500310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224678</td>\n",
       "      <td>0.514864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3936</th>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.731514</td>\n",
       "      <td>0.140167</td>\n",
       "      <td>0.749021</td>\n",
       "      <td>0.750316</td>\n",
       "      <td>0.031328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851758</td>\n",
       "      <td>0.450602</td>\n",
       "      <td>0.528139</td>\n",
       "      <td>0.605051</td>\n",
       "      <td>0.601097</td>\n",
       "      <td>0.446412</td>\n",
       "      <td>0.684836</td>\n",
       "      <td>0.074063</td>\n",
       "      <td>0.170475</td>\n",
       "      <td>0.413623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8798</th>\n",
       "      <td>0.529371</td>\n",
       "      <td>0.278337</td>\n",
       "      <td>0.084118</td>\n",
       "      <td>0.626482</td>\n",
       "      <td>0.848247</td>\n",
       "      <td>0.239914</td>\n",
       "      <td>0.029636</td>\n",
       "      <td>0.871150</td>\n",
       "      <td>0.565451</td>\n",
       "      <td>0.569982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407657</td>\n",
       "      <td>0.252476</td>\n",
       "      <td>0.523529</td>\n",
       "      <td>0.654260</td>\n",
       "      <td>0.860068</td>\n",
       "      <td>0.551171</td>\n",
       "      <td>0.780504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575375</td>\n",
       "      <td>0.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8383</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.041414</td>\n",
       "      <td>0.496126</td>\n",
       "      <td>0.915666</td>\n",
       "      <td>0.337744</td>\n",
       "      <td>0.167310</td>\n",
       "      <td>0.481890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371185</td>\n",
       "      <td>0.467613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.797838</td>\n",
       "      <td>0.230361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057428</td>\n",
       "      <td>0.142460</td>\n",
       "      <td>0.103106</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587226</td>\n",
       "      <td>0.556678</td>\n",
       "      <td>0.428475</td>\n",
       "      <td>0.360708</td>\n",
       "      <td>0.344114</td>\n",
       "      <td>0.503687</td>\n",
       "      <td>0.616717</td>\n",
       "      <td>0.200213</td>\n",
       "      <td>0.582584</td>\n",
       "      <td>0.630867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9487</th>\n",
       "      <td>0.706362</td>\n",
       "      <td>0.155848</td>\n",
       "      <td>0.100019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.742599</td>\n",
       "      <td>0.759649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223681</td>\n",
       "      <td>0.380915</td>\n",
       "      <td>0.811927</td>\n",
       "      <td>0.718884</td>\n",
       "      <td>0.443742</td>\n",
       "      <td>0.379867</td>\n",
       "      <td>0.292060</td>\n",
       "      <td>0.771193</td>\n",
       "      <td>0.227811</td>\n",
       "      <td>0.516097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.738235</td>\n",
       "      <td>0.240067</td>\n",
       "      <td>0.219303</td>\n",
       "      <td>0.307691</td>\n",
       "      <td>0.025989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322031</td>\n",
       "      <td>0.040017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387591</td>\n",
       "      <td>0.529386</td>\n",
       "      <td>0.572208</td>\n",
       "      <td>0.402661</td>\n",
       "      <td>0.296447</td>\n",
       "      <td>0.368112</td>\n",
       "      <td>0.693042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509002</td>\n",
       "      <td>0.323754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2092 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RPS4Y1      XIST      KRT5      AGR2   CEACAM5     KRT6A     KRT14  \\\n",
       "1980  0.000000  0.676411  0.538535  0.829060  0.371617  0.076709  0.493035   \n",
       "5638  0.000000  0.601335  0.791538  0.580519  0.711329  0.771928  0.557838   \n",
       "8240  0.000000  0.801651  0.515508  0.526579  0.119657  0.212524  0.086728   \n",
       "3555  0.767083  0.132404  0.000000  0.461636  0.000000  0.000000  0.000000   \n",
       "3936  0.022867  0.731514  0.140167  0.749021  0.750316  0.031328  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8798  0.529371  0.278337  0.084118  0.626482  0.848247  0.239914  0.029636   \n",
       "8383  0.000000  0.689733  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3652  0.067000  0.797838  0.230361  0.000000  0.057428  0.142460  0.103106   \n",
       "9487  0.706362  0.155848  0.100019  0.000000  0.000000  0.000000  0.021514   \n",
       "7526  0.000000  0.738235  0.240067  0.219303  0.307691  0.025989  0.000000   \n",
       "\n",
       "       CEACAM6     DDX3Y     KDM5D  ...   FAM129A   C8orf48    CDK5R1  \\\n",
       "1980  0.495491  0.000000  0.000000  ...  0.641401  0.258891  0.520563   \n",
       "5638  0.656916  0.000000  0.000000  ...  0.608653  0.157189  0.526314   \n",
       "8240  0.253842  0.000000  0.000000  ...  0.278018  0.160775  0.496794   \n",
       "3555  0.046623  0.774512  0.736268  ...  0.359250  0.657670  0.340666   \n",
       "3936  0.716501  0.000000  0.000000  ...  0.851758  0.450602  0.528139   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8798  0.871150  0.565451  0.569982  ...  0.407657  0.252476  0.523529   \n",
       "8383  0.000000  0.000000  0.000000  ...  0.586353  0.041414  0.496126   \n",
       "3652  0.024850  0.033195  0.033241  ...  0.587226  0.556678  0.428475   \n",
       "9487  0.000000  0.742599  0.759649  ...  0.223681  0.380915  0.811927   \n",
       "7526  0.322031  0.040017  0.000000  ...  0.387591  0.529386  0.572208   \n",
       "\n",
       "        FAM81A  C13orf18     GDPD3     SMAGP   C2orf85   POU5F1B     CHST2  \n",
       "1980  0.400908  0.437579  0.560402  0.483713  0.054940  0.333915  0.313550  \n",
       "5638  0.651298  0.422983  0.648942  0.718139  0.030541  0.219439  0.447640  \n",
       "8240  0.384108  0.434125  0.476615  0.634136  0.000000  0.433654  0.308404  \n",
       "3555  0.708832  0.358783  0.328927  0.500310  0.000000  0.224678  0.514864  \n",
       "3936  0.605051  0.601097  0.446412  0.684836  0.074063  0.170475  0.413623  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8798  0.654260  0.860068  0.551171  0.780504  0.000000  0.575375  0.269600  \n",
       "8383  0.915666  0.337744  0.167310  0.481890  0.000000  0.371185  0.467613  \n",
       "3652  0.360708  0.344114  0.503687  0.616717  0.200213  0.582584  0.630867  \n",
       "9487  0.718884  0.443742  0.379867  0.292060  0.771193  0.227811  0.516097  \n",
       "7526  0.402661  0.296447  0.368112  0.693042  0.000000  0.509002  0.323754  \n",
       "\n",
       "[2092 rows x 5000 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnaseq_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d52df-ed47-4777-91e8-f4514fcaa242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
